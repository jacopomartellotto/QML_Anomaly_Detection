{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6eebba",
   "metadata": {},
   "source": [
    "# ðŸ¤– Classic ML Models with Benchmark Evaluation ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454fcd8a",
   "metadata": {
    "papermill": {
     "duration": 0.032437,
     "end_time": "2023-07-29T17:54:40.831334",
     "exception": false,
     "start_time": "2023-07-29T17:54:40.798897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-zero\"></a>\n",
    "# Introduction\n",
    "Banks are often exposed to fraudulent transactions and they're constantly improving systems to track them. My goal is to train eight (8) machine learning models that will detect those fraudulent transactions.\n",
    "\n",
    "The models will be trained using a bank dataset that contains 20k+ transactions with 112 anonymized numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ba6c6",
   "metadata": {
    "_kg_hide-output": false,
    "papermill": {
     "duration": 0.048015,
     "end_time": "2023-07-29T17:54:40.912504",
     "exception": false,
     "start_time": "2023-07-29T17:54:40.864489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28216db",
   "metadata": {
    "papermill": {
     "duration": 0.032572,
     "end_time": "2023-07-29T17:54:40.977445",
     "exception": false,
     "start_time": "2023-07-29T17:54:40.944873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read and display data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06e270",
   "metadata": {
    "papermill": {
     "duration": 0.298046,
     "end_time": "2023-07-29T17:54:41.307788",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.009742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set working directory and read data\n",
    "data = pd.read_csv('dataset/fraud_detection_bank_dataset.csv') \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c3da6",
   "metadata": {
    "papermill": {
     "duration": 0.033053,
     "end_time": "2023-07-29T17:54:41.374318",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.341265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Drop the first column and rename the last column. Check for any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f4bd8",
   "metadata": {
    "papermill": {
     "duration": 0.059074,
     "end_time": "2023-07-29T17:54:41.467284",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.408210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the 1st column (Unnamed).\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "# Rename \"targets\" column to \"is_fraud\".\n",
    "data.rename(columns={'targets': 'is_fraud'}, inplace=True) \n",
    "\n",
    "# Check for any null values in each column\n",
    "print(f'Null Values: {sum(data.isnull().sum())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beced6b4",
   "metadata": {
    "papermill": {
     "duration": 0.033832,
     "end_time": "2023-07-29T17:54:41.534729",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.500897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Determine the class balance between non-fraudulent and fraudulent objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d677cd",
   "metadata": {
    "papermill": {
     "duration": 0.04842,
     "end_time": "2023-07-29T17:54:41.616600",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.568180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the number of fraud & non-fraud objects.\n",
    "class_balance = data.is_fraud.value_counts()\n",
    "fraud_percentage = ((class_balance[1]/len(data))*100)\n",
    "\n",
    "print(f'Non-Fraud: {class_balance[0]} '\n",
    "      f'({(100-fraud_percentage).__round__(4)}%)')\n",
    "print(f'Fraud: {class_balance[1]} '\n",
    "      f'({fraud_percentage.__round__(4)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corrmat = data.corr()\n",
    "\n",
    "# Select the top 30 features most correlated with 'is_fraud'\n",
    "k = 30\n",
    "cols = corrmat.nlargest(k, 'is_fraud')['is_fraud'].index\n",
    "\n",
    "# Compute the correlation matrix for these selected features\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "\n",
    "# Set the style and generate the heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.set(font_scale=1.1)\n",
    "sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "            annot_kws={'size': 9}, yticklabels=cols.values, xticklabels=cols.values,\n",
    "            cmap='coolwarm')\n",
    "\n",
    "plt.title('Correlation Among the Top 30 Features Most Related to \"is_fraud\"')\n",
    "plt.savefig('img/correlation_heatmap_top_30_features.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138c561",
   "metadata": {
    "papermill": {
     "duration": 0.033032,
     "end_time": "2023-07-29T17:54:41.683473",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.650441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Separate dataset into features (inputs) and targets (outputs). Verify all features are numerical and split dataset into training (80%) and testing (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300135ad",
   "metadata": {
    "papermill": {
     "duration": 1.056971,
     "end_time": "2023-07-29T17:54:42.774423",
     "exception": false,
     "start_time": "2023-07-29T17:54:41.717452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab53fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X = data.drop('is_fraud', axis=1)\n",
    "y = data['is_fraud']\n",
    "X, y = smote.fit_resample(X, y)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383243b6",
   "metadata": {
    "papermill": {
     "duration": 0.033017,
     "end_time": "2023-07-29T17:54:42.948024",
     "exception": false,
     "start_time": "2023-07-29T17:54:42.915007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize functions that later will be used to analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdda16",
   "metadata": {
    "papermill": {
     "duration": 0.041648,
     "end_time": "2023-07-29T17:54:43.023155",
     "exception": false,
     "start_time": "2023-07-29T17:54:42.981507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_indices(lst, val):\n",
    "    '''\n",
    "    Input: \n",
    "        lst: a list of elements (string, integer, float, etc).\n",
    "        val: the value being searched for in the list.\n",
    "        \n",
    "    Output: \n",
    "        indices: list of all indices that are equal to val.\n",
    "    '''\n",
    "    # Gets all the indices of a element in a list.\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == val:\n",
    "            indices.append(i)\n",
    "            \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ddb56",
   "metadata": {
    "papermill": {
     "duration": 0.044556,
     "end_time": "2023-07-29T17:54:43.100724",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.056168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bestParam(param_values, train_acc, test_acc):\n",
    "    '''\n",
    "    Input: \n",
    "        param_values: a list containing all of the parameter values tried\n",
    "        train_acc: a list containing the training accuracy for a model trained \n",
    "                   with each parameter in param_values.\n",
    "        test_acc: a list containing testing accuracy for a model trained with \n",
    "                  each parameter in param_values.\n",
    "        \n",
    "    Output: \n",
    "        best_param: the alpha value that produces the highest test_acc with \n",
    "                    the minimum difference (train_acc - test_acc).\n",
    "        best_train_acc: the training value that is the closest to the best_test_acc.\n",
    "        best_test_acc: the highest testing accuracy.\n",
    "    '''\n",
    "    # Gets all the indices of the maximum test accuracy value.\n",
    "    indices = get_indices(test_acc, max(test_acc))\n",
    "\n",
    "    diff = []\n",
    "    params = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    for i in indices:\n",
    "        # Calculate the difference between the training and testing \n",
    "        # accuracy based on the indices.\n",
    "        diff.append(abs(train_acc[i] - test_acc[i]))\n",
    "        params.append(param_values[i])\n",
    "        train_accuracy.append(train_acc[i])\n",
    "        test_accuracy.append(test_acc[i])\n",
    "        \n",
    "    # Get the index of the minimum difference in the list.\n",
    "    min_diff_idx = diff.index(min(diff))\n",
    "    # Get the best param using the minimum difference index.\n",
    "    best_param = params[min_diff_idx]\n",
    "    # Get the best training accuracy using the minimum difference index.\n",
    "    best_train_acc = train_accuracy[min_diff_idx] \n",
    "    # Get the best testing accuracy using the minimum difference index.\n",
    "    best_test_acc = test_accuracy[min_diff_idx] \n",
    "    \n",
    "    return best_param, best_train_acc, best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d49cab",
   "metadata": {
    "papermill": {
     "duration": 0.04249,
     "end_time": "2023-07-29T17:54:43.176462",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.133972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def disp_confMatrix():\n",
    "    cm = confusion_matrix(y_test, y_test_predicted)\n",
    "    cmp = ConfusionMatrixDisplay(cm, display_labels=target_names)\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944de044",
   "metadata": {
    "papermill": {
     "duration": 0.04556,
     "end_time": "2023-07-29T17:54:43.255595",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.210035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_metrics():\n",
    "    # Calculate accuracy for training and testing data.\n",
    "    train_accs.append(accuracy_score(y_train, y_train_predicted).__round__(4))\n",
    "    test_accs.append(accuracy_score(y_test, y_test_predicted).__round__(4))\n",
    "\n",
    "    # Calculate F1 score for training and testing data.\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_predicted).__round__(4))\n",
    "    test_f1_scores.append(f1_score(y_test, y_test_predicted).__round__(4))\n",
    "\n",
    "    # Obtain the false-positives and false-negatives from the testing data.\n",
    "    false_pos.append(confusion_matrix(y_test, y_test_predicted)[0,1]) \n",
    "    false_neg.append(confusion_matrix(y_test, y_test_predicted)[1,0]) \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bad727",
   "metadata": {
    "papermill": {
     "duration": 0.033052,
     "end_time": "2023-07-29T17:54:43.322503",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.289451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize lists for storing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5710e04",
   "metadata": {
    "papermill": {
     "duration": 0.047427,
     "end_time": "2023-07-29T17:54:43.403436",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.356009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_names = ['non-fraud (0)', 'fraud (1)'] # Used for classification report.\n",
    "false_pos = [] # Create list for false-positives.\n",
    "false_neg = [] # Create list for false-negatives.\n",
    "train_f1_scores = [] # Create list for training data F1 scores.\n",
    "test_f1_scores = []  # Create list for testing data F1 scores.\n",
    "train_accs = [] # Create list for training data accuracies.\n",
    "test_accs = []  # Create list for testing data accuracies.\n",
    "clf_names = ['Dummy', 'Decision Tree', 'AdaBoost', 'KNN', 'MLP', \n",
    "             'Logistic\\nRegression', 'Random\\nForest', 'SVM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60a83d",
   "metadata": {
    "papermill": {
     "duration": 0.036916,
     "end_time": "2023-07-29T17:54:43.479702",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.442786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# Training & Testing ML Models\n",
    "\n",
    "I will be training and testing eight (8) different ML models using *k*-fold cross-validation (CV) with hyper-parameter tuning to reduce the chances of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b277d",
   "metadata": {
    "papermill": {
     "duration": 0.043184,
     "end_time": "2023-07-29T17:54:43.556138",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.512954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Cross-validation value.\n",
    "cv = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e688b8",
   "metadata": {
    "papermill": {
     "duration": 0.041282,
     "end_time": "2023-07-29T17:54:43.632850",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.591568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-two\"></a>\n",
    "## Dummy Classifier\n",
    "This model was configured to always pick the majority class. I used this as a baseline to determine if the other models were doing better than a naive approach that wouldn't require any real training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def032c7",
   "metadata": {
    "papermill": {
     "duration": 0.047888,
     "end_time": "2023-07-29T17:54:43.720923",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.673035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2fec4",
   "metadata": {
    "papermill": {
     "duration": 0.033797,
     "end_time": "2023-07-29T17:54:43.788749",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.754952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Train and fit data to the model. Predict training and testing class. Display the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed8ac5",
   "metadata": {
    "papermill": {
     "duration": 0.079339,
     "end_time": "2023-07-29T17:54:43.901848",
     "exception": false,
     "start_time": "2023-07-29T17:54:43.822509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "dum = DummyClassifier(strategy='most_frequent', random_state=random_seed)\n",
    "dum.fit(X_train, y_train)\n",
    "\n",
    "# Predict training & testing class.\n",
    "y_train_predicted = dum.predict(X_train)\n",
    "y_test_predicted = dum.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f\"Classification Report with strategy = 'most_frequent'\\n\")\n",
    "print(f'\\t\\t    Dummy Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print(f'\\t\\t    Dummy Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f40b57",
   "metadata": {
    "papermill": {
     "duration": 0.033747,
     "end_time": "2023-07-29T17:54:44.367956",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.334209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Calculate probability score, precision and recall. Then, plot the AUC-PR.\n",
    "\n",
    "*Note: AUC-PR stands for area under the (precision-recall) curve. Generally, the higher the AUC-PR score, the better a classifier performs for the given task.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196272ca",
   "metadata": {
    "papermill": {
     "duration": 0.283912,
     "end_time": "2023-07-29T17:54:44.685538",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.401626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = dum.predict_proba(X_test)[:, 1] \n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "dum_precision, dum_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "dum_auc = auc(dum_recall, dum_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(15,8))            \n",
    "plt.plot(dum_recall, dum_precision, color='black', linestyle='--')\n",
    "plt.legend([f'Dummy (AUC = {dum_auc})'])\n",
    "plt.title(f'Dummy Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5d7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T18:11:12.690419Z",
     "iopub.status.busy": "2022-06-18T18:11:12.689946Z",
     "iopub.status.idle": "2022-06-18T18:11:12.695774Z",
     "shell.execute_reply": "2022-06-18T18:11:12.694829Z",
     "shell.execute_reply.started": "2022-06-18T18:11:12.690385Z"
    },
    "papermill": {
     "duration": 0.034122,
     "end_time": "2023-07-29T17:54:44.754636",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.720514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Store metrics to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f97cb1",
   "metadata": {
    "papermill": {
     "duration": 0.053925,
     "end_time": "2023-07-29T17:54:44.843210",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.789285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bb058",
   "metadata": {
    "papermill": {
     "duration": 0.034034,
     "end_time": "2023-07-29T17:54:44.911492",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.877458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-three\"></a>\n",
    "## Decision Tree Classifier\n",
    "### Choosing a `ccp_alpha` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc35105",
   "metadata": {
    "papermill": {
     "duration": 0.16886,
     "end_time": "2023-07-29T17:54:45.115810",
     "exception": false,
     "start_time": "2023-07-29T17:54:44.946950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14f8b6",
   "metadata": {
    "papermill": {
     "duration": 161.861231,
     "end_time": "2023-07-29T17:57:27.012088",
     "exception": false,
     "start_time": "2023-07-29T17:54:45.150857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model.\n",
    "dtc = DecisionTreeClassifier(criterion=\"gini\", random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "alpha_values = np.arange(0, 0.0025, 0.00001)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(dtc, X_train, y_train, \n",
    "                                              param_name='ccp_alpha', \n",
    "                                              param_range=alpha_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_alpha, best_train, best_valid = get_bestParam(alpha_values, train_scores, \n",
    "                                                   valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Alpha: {best_alpha}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Alpha for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(alpha_values, train_scores, color='blue')\n",
    "plt.plot(alpha_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title(f'Decision Tree Classifier - Accuracy vs Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af81a66",
   "metadata": {
    "papermill": {
     "duration": 0.478308,
     "end_time": "2023-07-29T17:57:27.531615",
     "exception": false,
     "start_time": "2023-07-29T17:57:27.053307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "dtc = DecisionTreeClassifier(criterion='gini', random_state=random_seed, \n",
    "                             ccp_alpha=best_alpha)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = dtc.predict(X_train)\n",
    "y_test_predicted = dtc.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with ccp_alpha = {best_alpha}\\n')\n",
    "print('\\t\\tDecision Tree Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\tDecision Tree Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925fe2d",
   "metadata": {
    "papermill": {
     "duration": 0.289136,
     "end_time": "2023-07-29T17:57:28.203772",
     "exception": false,
     "start_time": "2023-07-29T17:57:27.914636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = dtc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "dtc_precision, dtc_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "dtc_auc = auc(dtc_recall, dtc_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(dtc_recall, dtc_precision, color='blue')\n",
    "plt.legend([f'Decision Tree (AUC = {dtc_auc})'])\n",
    "plt.title(f'Decision Tree Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c01f0",
   "metadata": {
    "papermill": {
     "duration": 0.038644,
     "end_time": "2023-07-29T17:57:28.282491",
     "exception": false,
     "start_time": "2023-07-29T17:57:28.243847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `ccp_alpha` value that would produce the highest testing F1 score.\n",
    "\n",
    "The F1 score (also known as balanced F-score or F-measure) can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The F1 score is dependent upon recall and precision. The higher the recall and precision, the higher the F1 score. To increase recall and precision, I must reduce false-negatives and false-positives.\n",
    "\n",
    "Recall = TP/(TP + FN)<br> \n",
    "Precision = TP/(TP + FP)<br> \n",
    "\n",
    "F1 Score = 2 x (Recall x Precision)/(Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c023788",
   "metadata": {
    "papermill": {
     "duration": 83.695279,
     "end_time": "2023-07-29T17:58:52.016999",
     "exception": false,
     "start_time": "2023-07-29T17:57:28.321720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = [] \n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    dtc = DecisionTreeClassifier(criterion='gini', random_state=random_seed, \n",
    "                                 ccp_alpha=alpha)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = dtc.predict(X_train)\n",
    "    y_test_predicted = dtc.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.\n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338c7ac",
   "metadata": {
    "papermill": {
     "duration": 0.282194,
     "end_time": "2023-07-29T17:58:52.335707",
     "exception": false,
     "start_time": "2023-07-29T17:58:52.053513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_alpha, best_train, best_test = get_bestParam(alpha_values, fraud_train_f1_score, \n",
    "                                                  fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Alpha: {best_alpha}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Alpha for testing and training data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(alpha_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(alpha_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title(f'Decision Tree Classifier - Fraud F1 Score vs Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel(f'Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bcffb",
   "metadata": {
    "papermill": {
     "duration": 0.402111,
     "end_time": "2023-07-29T17:58:52.774765",
     "exception": false,
     "start_time": "2023-07-29T17:58:52.372654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "dtc = DecisionTreeClassifier(criterion='gini', random_state=random_seed, \n",
    "                             ccp_alpha=best_alpha)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict training & testing class.\n",
    "y_train_predicted = dtc.predict(X_train)\n",
    "y_test_predicted = dtc.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with ccp_alpha = {best_alpha}\\n')\n",
    "print('\\t\\tDecision Tree Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\tDecision Tree Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242c3d9",
   "metadata": {
    "papermill": {
     "duration": 0.259935,
     "end_time": "2023-07-29T17:58:53.372855",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.112920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = dtc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "dtc_precision, dtc_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "dtc_auc = auc(dtc_recall, dtc_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(dtc_recall, dtc_precision, color='blue')\n",
    "plt.legend([f'Decision Tree (AUC = {dtc_auc})'])\n",
    "plt.title(f'Decision Tree Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459478b",
   "metadata": {
    "papermill": {
     "duration": 0.058186,
     "end_time": "2023-07-29T17:58:53.469865",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.411679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af72dd",
   "metadata": {
    "papermill": {
     "duration": 0.037192,
     "end_time": "2023-07-29T17:58:53.544558",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.507366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As result of choosing an alpha that would produce the highest testing F1 score, I was able to reduce FP and FN while increasing TP & TN. \n",
    "\n",
    "- FP decreased by **3.21%**\n",
    "- FN decreased by **9.14%**\n",
    "- TP increased by **2.03%**\n",
    "- TN increased by **0.18%** \n",
    "\n",
    "#### Training Metrics\n",
    "- Precision increased by **0.66%**<br>\n",
    "- Recall increased by **1.07%**<br>\n",
    "- F1 score increased by **0.87%**<br>\n",
    "\n",
    "#### Testing Metrics\n",
    "- Precision increased by **0.78%**<br>\n",
    "- Recall increased by **2.03**%<br>\n",
    "- F1 score increased by **1.42%**<br>\n",
    "\n",
    "I initially chose `ccp_alpha` based on the accuracy of the training and validation data. However, when choosing `ccp_alpha`  based on the F1 score, it not only improved recall and precision but it also improved the accuracy by **0.61%**. Moving forward, I will compare these two (2) methods and choose the model that performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19438c",
   "metadata": {
    "papermill": {
     "duration": 0.03678,
     "end_time": "2023-07-29T17:58:53.618054",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.581274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-four\"></a>\n",
    "## AdaBoost Classifier\n",
    "### Choosing a `n_estimators` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e45df",
   "metadata": {
    "papermill": {
     "duration": 0.093027,
     "end_time": "2023-07-29T17:58:53.747871",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.654844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f1f1e",
   "metadata": {
    "papermill": {
     "duration": 8737.244441,
     "end_time": "2023-07-29T20:24:31.029316",
     "exception": false,
     "start_time": "2023-07-29T17:58:53.784875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "ada = AdaBoostClassifier(random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "estimator_values = np.arange(1, 500, 1)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(ada, X_train, y_train, \n",
    "                                              param_name='n_estimators', \n",
    "                                              param_range=estimator_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_estimator, best_train, best_valid = get_bestParam(estimator_values, train_scores, \n",
    "                                                       valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Estimator: {best_estimator}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Estimators for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(estimator_values, train_scores, color='blue')\n",
    "plt.plot(estimator_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title('AdaBoost Classifier - Accuracy vs Estimators')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea84688",
   "metadata": {
    "papermill": {
     "duration": 11.006111,
     "end_time": "2023-07-29T20:24:42.073244",
     "exception": false,
     "start_time": "2023-07-29T20:24:31.067133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "ada = AdaBoostClassifier(n_estimators=best_estimator, random_state=random_seed)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = ada.predict(X_train)\n",
    "y_test_predicted = ada.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with n_estimators = {best_estimator}\\n')\n",
    "print('\\t\\t  AdaBoost Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  AdaBoost Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3ff71",
   "metadata": {
    "papermill": {
     "duration": 0.61161,
     "end_time": "2023-07-29T20:24:43.024759",
     "exception": false,
     "start_time": "2023-07-29T20:24:42.413149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = ada.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "ada_precision, ada_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "ada_auc = auc(ada_recall, ada_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(ada_recall, ada_precision, color='orange')\n",
    "plt.legend([f'AdaBoost (AUC = {ada_auc})'])\n",
    "plt.title('AdaBoost Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef654f87",
   "metadata": {
    "papermill": {
     "duration": 0.038315,
     "end_time": "2023-07-29T20:24:43.105055",
     "exception": false,
     "start_time": "2023-07-29T20:24:43.066740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `n_estimators` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076cc59",
   "metadata": {
    "papermill": {
     "duration": 4108.898518,
     "end_time": "2023-07-29T21:33:12.042416",
     "exception": false,
     "start_time": "2023-07-29T20:24:43.143898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for estimators in estimator_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    ada = AdaBoostClassifier(n_estimators=estimators, random_state=random_seed)\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = ada.predict(X_train)\n",
    "    y_test_predicted = ada.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.    \n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4267a3",
   "metadata": {
    "papermill": {
     "duration": 0.289455,
     "end_time": "2023-07-29T21:33:12.372024",
     "exception": false,
     "start_time": "2023-07-29T21:33:12.082569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_estimator, best_train, best_test = get_bestParam(\n",
    "    estimator_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Estimator: {best_estimator}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Estimators for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(estimator_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(estimator_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('AdaBoost Classifier -  Fraud F1 Score vs Estimators')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel(f'Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0bf30",
   "metadata": {
    "papermill": {
     "duration": 2.793658,
     "end_time": "2023-07-29T21:33:15.205495",
     "exception": false,
     "start_time": "2023-07-29T21:33:12.411837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "ada = AdaBoostClassifier(n_estimators=best_estimator, random_state=random_seed)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = ada.predict(X_train)\n",
    "y_test_predicted = ada.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with n_estimators = {best_estimator}\\n')\n",
    "print('\\t\\t  AdaBoost Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  AdaBoost Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ee319",
   "metadata": {
    "papermill": {
     "duration": 0.356316,
     "end_time": "2023-07-29T21:33:15.902868",
     "exception": false,
     "start_time": "2023-07-29T21:33:15.546552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = ada.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "ada_precision, ada_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "ada_auc = auc(ada_recall, ada_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(ada_recall, ada_precision, color='orange')\n",
    "plt.legend([f'AdaBoost (AUC = {ada_auc})'])\n",
    "plt.title('AdaBoost Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97795134",
   "metadata": {
    "papermill": {
     "duration": 0.059007,
     "end_time": "2023-07-29T21:33:16.001949",
     "exception": false,
     "start_time": "2023-07-29T21:33:15.942942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8a620",
   "metadata": {
    "papermill": {
     "duration": 0.039846,
     "end_time": "2023-07-29T21:33:16.081660",
     "exception": false,
     "start_time": "2023-07-29T21:33:16.041814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-five\"></a>\n",
    "## K Nearest Neighbors (KNN) Classifier\n",
    "### Choosing a `n_neighbors` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c03e9",
   "metadata": {
    "papermill": {
     "duration": 0.04612,
     "end_time": "2023-07-29T21:33:16.167198",
     "exception": false,
     "start_time": "2023-07-29T21:33:16.121078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6234f80",
   "metadata": {
    "papermill": {
     "duration": 1311.771172,
     "end_time": "2023-07-29T21:55:07.978888",
     "exception": false,
     "start_time": "2023-07-29T21:33:16.207716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "neighbor_values = np.arange(1, 101, 1)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(knn, X_train, y_train, \n",
    "                                              param_name='n_neighbors', \n",
    "                                              param_range=neighbor_values, cv=cv)\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_neighbor, best_train, best_valid = get_bestParam(neighbor_values, train_scores, \n",
    "                                                      valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Neighbor: {best_neighbor}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Neighbors for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(neighbor_values, train_scores, color='blue')\n",
    "plt.plot(neighbor_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title('KNN Classifier - Accuracy vs Neighbors')\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16e40d",
   "metadata": {
    "papermill": {
     "duration": 7.361838,
     "end_time": "2023-07-29T21:55:15.381339",
     "exception": false,
     "start_time": "2023-07-29T21:55:08.019501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "knn = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with n_neighbors = {best_neighbor}\\n')\n",
    "print('\\t\\t     KNN Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t     KNN Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adcab5",
   "metadata": {
    "papermill": {
     "duration": 1.66756,
     "end_time": "2023-07-29T21:55:17.398825",
     "exception": false,
     "start_time": "2023-07-29T21:55:15.731265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "knn_precision, knn_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "knn_auc = auc(knn_recall, knn_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(knn_recall, knn_precision, color='magenta')\n",
    "plt.legend([f'KNN (AUC = {knn_auc})'])\n",
    "plt.title('KNN Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd65c7",
   "metadata": {
    "papermill": {
     "duration": 0.041905,
     "end_time": "2023-07-29T21:55:17.482980",
     "exception": false,
     "start_time": "2023-07-29T21:55:17.441075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `n_neighbors` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee85834",
   "metadata": {
    "papermill": {
     "duration": 713.314877,
     "end_time": "2023-07-29T22:07:10.839449",
     "exception": false,
     "start_time": "2023-07-29T21:55:17.524572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for neighbors in neighbor_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = knn.predict(X_train)\n",
    "    y_test_predicted = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.    \n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ddb14",
   "metadata": {
    "papermill": {
     "duration": 0.288831,
     "end_time": "2023-07-29T22:07:11.170774",
     "exception": false,
     "start_time": "2023-07-29T22:07:10.881943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_neighbor, best_train, best_test = get_bestParam(\n",
    "    neighbor_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Neighbors: {best_neighbor}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Neighbors for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(neighbor_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(neighbor_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('KNN Classifier - Fraud F1 Score vs Neighbors')\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel('Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31800ef",
   "metadata": {
    "papermill": {
     "duration": 7.001018,
     "end_time": "2023-07-29T22:07:18.214865",
     "exception": false,
     "start_time": "2023-07-29T22:07:11.213847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "knn = KNeighborsClassifier(n_neighbors=best_neighbor)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with n_neighbors = {best_neighbor}\\n')\n",
    "print('\\t\\t     KNN Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t     KNN Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f6079",
   "metadata": {
    "papermill": {
     "duration": 1.582349,
     "end_time": "2023-07-29T22:07:20.164740",
     "exception": false,
     "start_time": "2023-07-29T22:07:18.582391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "knn_precision, knn_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "knn_auc = auc(knn_recall, knn_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(knn_recall, knn_precision, color='magenta')\n",
    "plt.legend([f'KNN (AUC = {knn_auc})'])\n",
    "plt.title('KNN Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2184961",
   "metadata": {
    "papermill": {
     "duration": 0.06196,
     "end_time": "2023-07-29T22:07:20.270327",
     "exception": false,
     "start_time": "2023-07-29T22:07:20.208367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bec11c",
   "metadata": {
    "papermill": {
     "duration": 0.042505,
     "end_time": "2023-07-29T22:07:20.355875",
     "exception": false,
     "start_time": "2023-07-29T22:07:20.313370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-six\"></a>\n",
    "## Multi-Layer Perceptron (MLP) Classifier\n",
    "### Choosing a `max_iter` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a27c0",
   "metadata": {
    "papermill": {
     "duration": 0.058505,
     "end_time": "2023-07-29T22:07:20.457466",
     "exception": false,
     "start_time": "2023-07-29T22:07:20.398961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e534b",
   "metadata": {
    "papermill": {
     "duration": 1269.640991,
     "end_time": "2023-07-29T22:28:30.141830",
     "exception": false,
     "start_time": "2023-07-29T22:07:20.500839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10), activation='tanh',\n",
    "                    random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "max_iter_values = np.arange(10, 150, 1)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(mlp, X_train, y_train, \n",
    "                                              param_name='max_iter', \n",
    "                                              param_range=max_iter_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_max_iter, best_train, best_valid = get_bestParam(max_iter_values, train_scores, \n",
    "                                                      valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, train_scores, color='blue')\n",
    "plt.plot(max_iter_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title('MLP Classifier - Accuracy vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2fb95",
   "metadata": {
    "papermill": {
     "duration": 5.48227,
     "end_time": "2023-07-29T22:28:35.668026",
     "exception": false,
     "start_time": "2023-07-29T22:28:30.185756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10), activation='tanh',\n",
    "                    max_iter=best_max_iter, random_state=random_seed)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = mlp.predict(X_train)\n",
    "y_test_predicted = mlp.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t  MLP Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  MLP Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6f98c",
   "metadata": {
    "papermill": {
     "duration": 0.287058,
     "end_time": "2023-07-29T22:28:36.341835",
     "exception": false,
     "start_time": "2023-07-29T22:28:36.054777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "mlp_precision, mlp_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "mlp_auc = auc(mlp_recall, mlp_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(mlp_recall, mlp_precision, color='green')\n",
    "plt.legend([f'MLP (AUC = {mlp_auc})'])\n",
    "plt.title('MLP Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b7991",
   "metadata": {
    "papermill": {
     "duration": 0.044628,
     "end_time": "2023-07-29T22:28:36.432151",
     "exception": false,
     "start_time": "2023-07-29T22:28:36.387523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `max_iter` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc056b82",
   "metadata": {
    "papermill": {
     "duration": 589.371376,
     "end_time": "2023-07-29T22:38:25.847717",
     "exception": false,
     "start_time": "2023-07-29T22:28:36.476341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for max_iter in max_iter_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,10), activation='tanh',\n",
    "                    max_iter=max_iter, random_state=random_seed)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = mlp.predict(X_train)\n",
    "    y_test_predicted = mlp.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.    \n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218da53",
   "metadata": {
    "papermill": {
     "duration": 0.29324,
     "end_time": "2023-07-29T22:38:26.245775",
     "exception": false,
     "start_time": "2023-07-29T22:38:25.952535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_max_iter, best_train, best_test = get_bestParam(\n",
    "    max_iter_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(max_iter_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('MLP Classifier - Fraud F1 Score vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel('Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8060ee4",
   "metadata": {
    "papermill": {
     "duration": 4.510716,
     "end_time": "2023-07-29T22:38:30.802345",
     "exception": false,
     "start_time": "2023-07-29T22:38:26.291629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10), activation='tanh',\n",
    "                    max_iter=best_max_iter, random_state=random_seed)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = mlp.predict(X_train)\n",
    "y_test_predicted = mlp.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t  MLP Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  MLP Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffc84f",
   "metadata": {
    "papermill": {
     "duration": 0.29389,
     "end_time": "2023-07-29T22:38:31.494056",
     "exception": false,
     "start_time": "2023-07-29T22:38:31.200166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "mlp_precision, mlp_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "mlp_auc = auc(mlp_recall, mlp_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(mlp_recall, mlp_precision, color='green')\n",
    "plt.legend([f'MLP (AUC = {mlp_auc})'])\n",
    "plt.title('MLP Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba2cf7",
   "metadata": {
    "papermill": {
     "duration": 0.065959,
     "end_time": "2023-07-29T22:38:31.606804",
     "exception": false,
     "start_time": "2023-07-29T22:38:31.540845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5094d9",
   "metadata": {
    "papermill": {
     "duration": 0.046829,
     "end_time": "2023-07-29T22:38:31.700339",
     "exception": false,
     "start_time": "2023-07-29T22:38:31.653510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-seven\"></a>\n",
    "## Logistic Regression\n",
    "### Choosing a `max_iter` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486599d",
   "metadata": {
    "papermill": {
     "duration": 0.053884,
     "end_time": "2023-07-29T22:38:31.800283",
     "exception": false,
     "start_time": "2023-07-29T22:38:31.746399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dea924",
   "metadata": {
    "papermill": {
     "duration": 1740.434721,
     "end_time": "2023-07-29T23:07:32.281404",
     "exception": false,
     "start_time": "2023-07-29T22:38:31.846683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "lrg = LogisticRegression(solver='newton-cg', random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "max_iter_values = np.arange(1, 175, 1)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(lrg, X_train, y_train, \n",
    "                                              param_name='max_iter', \n",
    "                                              param_range=max_iter_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_max_iter, best_train, best_valid = get_bestParam(max_iter_values, train_scores, \n",
    "                                                      valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Cross Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, train_scores, color='blue')\n",
    "plt.plot(max_iter_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Cross Validation Score'])\n",
    "plt.title('Logistic Regression Classifier - Accuracy vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd54d1",
   "metadata": {
    "papermill": {
     "duration": 6.566285,
     "end_time": "2023-07-29T23:07:38.895498",
     "exception": false,
     "start_time": "2023-07-29T23:07:32.329213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "lrg = LogisticRegression(solver='newton-cg', random_state=random_seed, \n",
    "                         max_iter=best_max_iter)\n",
    "lrg.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = lrg.predict(X_train)\n",
    "y_test_predicted = lrg.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t  Logistic Regression (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  Logistic Regression (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fca0e0",
   "metadata": {
    "papermill": {
     "duration": 0.295848,
     "end_time": "2023-07-29T23:07:39.616062",
     "exception": false,
     "start_time": "2023-07-29T23:07:39.320214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = lrg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "lrg_precision, lrg_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "lrg_auc = auc(lrg_recall, lrg_precision).__round__(4)\n",
    "\n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(lrg_recall, lrg_recall, color='blueviolet')\n",
    "plt.legend([f'Logistic Regression (AUC = {lrg_auc})'])\n",
    "plt.title(f'Logistic Regression Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10ab46",
   "metadata": {
    "papermill": {
     "duration": 0.11583,
     "end_time": "2023-07-29T23:07:39.780704",
     "exception": false,
     "start_time": "2023-07-29T23:07:39.664874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `max_iter` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fbc6e",
   "metadata": {
    "papermill": {
     "duration": 860.072458,
     "end_time": "2023-07-29T23:21:59.901508",
     "exception": false,
     "start_time": "2023-07-29T23:07:39.829050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for max_iter in max_iter_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    lrg = LogisticRegression(solver='newton-cg', random_state=random_seed, \n",
    "                             max_iter=max_iter)\n",
    "    lrg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training & testing class.\n",
    "    y_train_predicted = lrg.predict(X_train)\n",
    "    y_test_predicted = lrg.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.    \n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f9d3f",
   "metadata": {
    "papermill": {
     "duration": 0.308553,
     "end_time": "2023-07-29T23:22:00.313813",
     "exception": false,
     "start_time": "2023-07-29T23:22:00.005260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_max_iter, best_train, best_test = get_bestParam(\n",
    "    max_iter_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(max_iter_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('Logistic Regression Classifier - Fraud F1 Score vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel(f'Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85a561",
   "metadata": {
    "papermill": {
     "duration": 6.910758,
     "end_time": "2023-07-29T23:22:07.272925",
     "exception": false,
     "start_time": "2023-07-29T23:22:00.362167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "lrg = LogisticRegression(solver='newton-cg', random_state=random_seed, \n",
    "                         max_iter=best_max_iter)\n",
    "lrg.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = lrg.predict(X_train)\n",
    "y_test_predicted = lrg.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t  Logistic Regression (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  Logistic Regression (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe8663",
   "metadata": {
    "papermill": {
     "duration": 0.294888,
     "end_time": "2023-07-29T23:22:07.966868",
     "exception": false,
     "start_time": "2023-07-29T23:22:07.671980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = lrg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "lrg_precision, lrg_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "lrg_auc = auc(lrg_recall, lrg_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(lrg_recall, lrg_precision, color='blueviolet')\n",
    "plt.legend([f'Logistic Regression (AUC = {lrg_auc})'])\n",
    "plt.title('Logistic Regression PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c0130",
   "metadata": {
    "papermill": {
     "duration": 0.06864,
     "end_time": "2023-07-29T23:22:08.085370",
     "exception": false,
     "start_time": "2023-07-29T23:22:08.016730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f481623",
   "metadata": {
    "papermill": {
     "duration": 0.049362,
     "end_time": "2023-07-29T23:22:08.183459",
     "exception": false,
     "start_time": "2023-07-29T23:22:08.134097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-eight\"></a>\n",
    "## Random Forest Classifier\n",
    "### Choosing a `max_iter` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150a177",
   "metadata": {
    "papermill": {
     "duration": 0.056211,
     "end_time": "2023-07-29T23:22:08.288836",
     "exception": false,
     "start_time": "2023-07-29T23:22:08.232625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fba6e8",
   "metadata": {
    "papermill": {
     "duration": 6773.650701,
     "end_time": "2023-07-30T01:15:01.988434",
     "exception": false,
     "start_time": "2023-07-29T23:22:08.337733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "rfc = RandomForestClassifier(random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "estimator_values = np.arange(1, 500, 1)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(rfc, X_train, y_train, \n",
    "                                              param_name='n_estimators', \n",
    "                                              param_range=estimator_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_estimator, best_train, best_valid = get_bestParam(estimator_values, train_scores, \n",
    "                                                       valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Estimator: {best_estimator}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Estimators for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(estimator_values, train_scores, color='blue')\n",
    "plt.plot(estimator_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title('Random Forest Classifier - Accuracy vs Estimators')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0ef1d",
   "metadata": {
    "papermill": {
     "duration": 3.654791,
     "end_time": "2023-07-30T01:15:05.693106",
     "exception": false,
     "start_time": "2023-07-30T01:15:02.038315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "rfc = RandomForestClassifier(n_estimators=best_estimator, random_state=random_seed)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = rfc.predict(X_train)\n",
    "y_test_predicted = rfc.predict(X_test)\n",
    "\n",
    "# Display classification report for training and testing data.\n",
    "print(f'Classification Report with n_estimators = {best_estimator}\\n')\n",
    "print('\\t\\t  Random Forest Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  Random Forest Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3835996",
   "metadata": {
    "papermill": {
     "duration": 0.388499,
     "end_time": "2023-07-30T01:15:06.453802",
     "exception": false,
     "start_time": "2023-07-30T01:15:06.065303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = rfc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "rfc_precision, rfc_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "rfc_auc = auc(rfc_recall, rfc_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(rfc_recall, rfc_precision, color='red')\n",
    "plt.legend([f'Random Forest (AUC = {rfc_auc})'])\n",
    "plt.title('Random Forest Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25f71b",
   "metadata": {
    "papermill": {
     "duration": 0.050425,
     "end_time": "2023-07-30T01:15:06.555465",
     "exception": false,
     "start_time": "2023-07-30T01:15:06.505040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `max_iter` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ebc8e",
   "metadata": {
    "papermill": {
     "duration": 3336.37067,
     "end_time": "2023-07-30T02:10:42.976088",
     "exception": false,
     "start_time": "2023-07-30T01:15:06.605418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for estimators in estimator_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    rfc = RandomForestClassifier(n_estimators=estimators, random_state=random_seed)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = rfc.predict(X_train)\n",
    "    y_test_predicted = rfc.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.\n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a54a0",
   "metadata": {
    "papermill": {
     "duration": 0.299693,
     "end_time": "2023-07-30T02:10:43.328702",
     "exception": false,
     "start_time": "2023-07-30T02:10:43.029009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_estimator, best_train, best_test = get_bestParam(\n",
    "    estimator_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Estimator: {best_estimator}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Fraud F1 Score vs Estimators for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(estimator_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(estimator_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('Random Forest Classifier - Fraud F1 Score vs Estimators')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel('Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becc8d1",
   "metadata": {
    "papermill": {
     "duration": 7.732588,
     "end_time": "2023-07-30T02:10:51.114019",
     "exception": false,
     "start_time": "2023-07-30T02:10:43.381431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "rfc = RandomForestClassifier(n_estimators=best_estimator, random_state=random_seed)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = rfc.predict(X_train)\n",
    "y_test_predicted = rfc.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with n_estimators = {best_estimator}\\n')\n",
    "print('\\t\\t  Random Forest Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t  Random Forest Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab130e25",
   "metadata": {
    "papermill": {
     "duration": 0.525238,
     "end_time": "2023-07-30T02:10:52.022951",
     "exception": false,
     "start_time": "2023-07-30T02:10:51.497713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = rfc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "rfc_precision, rfc_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "rfc_auc = auc(rfc_recall, rfc_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(rfc_recall, rfc_precision, color='red')\n",
    "plt.legend([f'Random Forest (AUC = {rfc_auc})'])\n",
    "plt.title('Random Forest Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dd288",
   "metadata": {
    "papermill": {
     "duration": 0.071874,
     "end_time": "2023-07-30T02:10:52.147641",
     "exception": false,
     "start_time": "2023-07-30T02:10:52.075767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d13f85",
   "metadata": {
    "papermill": {
     "duration": 0.051841,
     "end_time": "2023-07-30T02:10:52.251334",
     "exception": false,
     "start_time": "2023-07-30T02:10:52.199493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-nine\"></a>\n",
    "## Support Vector Machine (SVM)\n",
    "### Choosing a `max_iter` value that would produce the highest validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193393b7",
   "metadata": {
    "papermill": {
     "duration": 0.059694,
     "end_time": "2023-07-30T02:10:52.363524",
     "exception": false,
     "start_time": "2023-07-30T02:10:52.303830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934fcf9",
   "metadata": {
    "papermill": {
     "duration": 2685.860234,
     "end_time": "2023-07-30T02:55:38.275605",
     "exception": false,
     "start_time": "2023-07-30T02:10:52.415371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "svm = SVC(gamma='auto', random_state=random_seed)\n",
    "\n",
    "# Generates a list that starts at minimum, ends at maximum, and increments by step.\n",
    "max_iter_values = np.arange(1600, 3000, 20)\n",
    "\n",
    "# Calculate training and validation accuracies.\n",
    "train_scores, valid_scores = validation_curve(svm, X_train, y_train, \n",
    "                                              param_name='max_iter', \n",
    "                                              param_range=max_iter_values, cv=cv)\n",
    "\n",
    "# Average training and validation accuracy.\n",
    "train_scores = np.mean(train_scores, axis=1)\n",
    "valid_scores = np.mean(valid_scores, axis=1)\n",
    "\n",
    "# Obtain the best parameter value that would produce the highest validation accuracy.\n",
    "best_max_iter, best_train, best_valid = get_bestParam(max_iter_values, train_scores, \n",
    "                                                      valid_scores)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training Accuracy: {best_train}')\n",
    "print(f'Validation Accuracy: {best_valid}')\n",
    "\n",
    "# Plot Accuracy vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, train_scores, color='blue')\n",
    "plt.plot(max_iter_values, valid_scores, color='red')\n",
    "plt.legend(['Training Score','Validation Score'])\n",
    "plt.title('Support Vector Classifier - Accuracy vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel(f'Average Accuracy of {cv}-Fold Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b49b40",
   "metadata": {
    "papermill": {
     "duration": 68.632767,
     "end_time": "2023-07-30T02:56:46.961464",
     "exception": false,
     "start_time": "2023-07-30T02:55:38.328697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "svm = SVC(gamma='auto', random_state=random_seed, \n",
    "          max_iter=best_max_iter, probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = svm.predict(X_train)\n",
    "y_test_predicted = svm.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t   SVM Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t   SVM Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b346507",
   "metadata": {
    "papermill": {
     "duration": 3.247145,
     "end_time": "2023-07-30T02:56:50.603892",
     "exception": false,
     "start_time": "2023-07-30T02:56:47.356747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "svm_precision, svm_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "svm_auc = auc(svm_recall, svm_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(svm_recall, svm_precision, color='dodgerblue')\n",
    "plt.legend([f'SVM (AUC = {svm_auc})'])\n",
    "plt.title('SVM Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d264adb",
   "metadata": {
    "papermill": {
     "duration": 0.055307,
     "end_time": "2023-07-30T02:56:50.714002",
     "exception": false,
     "start_time": "2023-07-30T02:56:50.658695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choosing a `max_iter` value that would produce the highest testing F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a02e0a",
   "metadata": {
    "papermill": {
     "duration": 1347.352689,
     "end_time": "2023-07-30T03:19:18.122401",
     "exception": false,
     "start_time": "2023-07-30T02:56:50.769712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_train_f1_score = []\n",
    "fraud_test_f1_score = []\n",
    "\n",
    "for max_iter in max_iter_values:\n",
    "    # Create model and fit model to the training data.\n",
    "    svm = SVC(gamma='auto', random_state=random_seed, max_iter=max_iter)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict training and testing class.\n",
    "    y_train_predicted = svm.predict(X_train)\n",
    "    y_test_predicted = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for training and testing data.\n",
    "    fraud_train_f1_score.append((f1_score(y_train, y_train_predicted)).__round__(4))\n",
    "    fraud_test_f1_score.append((f1_score(y_test, y_test_predicted)).__round__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac4d93",
   "metadata": {
    "papermill": {
     "duration": 0.32093,
     "end_time": "2023-07-30T03:19:18.499279",
     "exception": false,
     "start_time": "2023-07-30T03:19:18.178349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the best parameter value that would produce the highest testing F1 score.\n",
    "best_max_iter, best_train, best_test = get_bestParam(\n",
    "    max_iter_values, fraud_train_f1_score, fraud_test_f1_score)\n",
    "\n",
    "# Display best parameter values.\n",
    "print(f'Best Max Iterations: {best_max_iter}')\n",
    "print(f'Training F1 Score: {best_train}')\n",
    "print(f'Testing F1 Score: {best_test}')\n",
    "\n",
    "# Plot Accuracy vs Max Iterations for training and testing data.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(max_iter_values, fraud_train_f1_score, color='blue')\n",
    "plt.plot(max_iter_values, fraud_test_f1_score, color='red')\n",
    "plt.legend(['Training F1 Score','Testing F1 Score'])\n",
    "plt.title('SVM Classifier - Fraud F1 Score vs Max Iterations')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel(f'Fraud F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54ecd",
   "metadata": {
    "papermill": {
     "duration": 63.004101,
     "end_time": "2023-07-30T03:20:21.567594",
     "exception": false,
     "start_time": "2023-07-30T03:19:18.563493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and fit model to the training data.\n",
    "svm = SVC(gamma='auto', random_state=random_seed, \n",
    "          max_iter=best_max_iter, probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict training and testing class.\n",
    "y_train_predicted = svm.predict(X_train)\n",
    "y_test_predicted = svm.predict(X_test)\n",
    "\n",
    "# Display classification report for training & testing data.\n",
    "print(f'Classification Report with max_iter = {best_max_iter}\\n')\n",
    "print('\\t\\t   SVM Classifier (Train)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_train, y_train_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')\n",
    "print('\\t\\t   SVM Classifier (Test)')\n",
    "print('\\t---------------------------------------------')\n",
    "print(classification_report(y_test, y_test_predicted, \n",
    "                            target_names=target_names, digits=4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3db4a",
   "metadata": {
    "papermill": {
     "duration": 2.9096,
     "end_time": "2023-07-30T03:20:25.002257",
     "exception": false,
     "start_time": "2023-07-30T03:20:22.092657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for testing data.\n",
    "y_score = svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds.\n",
    "svm_precision, svm_recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "# Calculate and plot AUC-PR.\n",
    "svm_auc = auc(svm_recall, svm_precision).__round__(4)\n",
    "              \n",
    "plt.figure(figsize=(9,6))            \n",
    "plt.plot(svm_recall, svm_precision, color='dodgerblue')\n",
    "plt.legend([f'SVM (AUC = {svm_auc})'])\n",
    "plt.title('SVM Classifier PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1572211",
   "metadata": {
    "papermill": {
     "duration": 0.078087,
     "end_time": "2023-07-30T03:20:25.136671",
     "exception": false,
     "start_time": "2023-07-30T03:20:25.058584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cae3a8",
   "metadata": {
    "papermill": {
     "duration": 0.056199,
     "end_time": "2023-07-30T03:20:25.250692",
     "exception": false,
     "start_time": "2023-07-30T03:20:25.194493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-ten\"></a>\n",
    "# Comparing ML Models\n",
    "\n",
    "I will be comparing different metrics (e.g., accuracy, F1 score and AUC-PR) to determine which model is the most efficient at predicting fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad608ba",
   "metadata": {
    "papermill": {
     "duration": 0.23754,
     "end_time": "2023-07-30T03:20:25.549600",
     "exception": false,
     "start_time": "2023-07-30T03:20:25.312060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter # Used in sorting procedure.\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad7e44",
   "metadata": {
    "papermill": {
     "duration": 0.056546,
     "end_time": "2023-07-30T03:20:25.663355",
     "exception": false,
     "start_time": "2023-07-30T03:20:25.606809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model vs Accuracy\n",
    "\n",
    "Compare training and testing accuracies for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142492d",
   "metadata": {
    "papermill": {
     "duration": 0.378918,
     "end_time": "2023-07-30T03:20:26.099067",
     "exception": false,
     "start_time": "2023-07-30T03:20:25.720149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = [] # Used to create list of tuples.\n",
    "\n",
    "# Create list of tuples with classifer name, training accuracy, and testing accuracy.\n",
    "for i in range(len(clf_names)):\n",
    "    D.append((clf_names[i], train_accs[i], test_accs[i]))\n",
    "\n",
    "# Sort list of tuples by test accuracy.\n",
    "Dsort = sorted(D, key=itemgetter(2), reverse=False) \n",
    "\n",
    "clf_names_0 = [x[0] for x in Dsort] # Recreate list with sorted classifier (model) names.\n",
    "train_accs = [x[1] for x in Dsort] # Recreate list with sorted training accuracies. \n",
    "test_accs = [x[2] for x in Dsort] # Recreate list with sorted testing accuracies. \n",
    "\n",
    "ind = np.arange(len(clf_names_0)) # Barchart index\n",
    "width = 0.28 # Barwidth\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = plt.subplot(111)\n",
    "ax.barh(ind, train_accs, width, align='center', color='pink', label='Training') \n",
    "ax.barh(ind - width, test_accs, width, align='center', color='red', label='Testing') \n",
    "ax.set(yticks=ind - width/2, yticklabels=clf_names_0, ylim=[2*width - 1, len(clf_names_0)])\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Fraud Detection - Model vs Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('img/fraud_detection_model_accuracy.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571cc50",
   "metadata": {
    "papermill": {
     "duration": 0.056942,
     "end_time": "2023-07-30T03:20:26.213601",
     "exception": false,
     "start_time": "2023-07-30T03:20:26.156659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Random Forest** model has the highest training and testing accuracy. The training accuracy is **0.9999** and the testing accuracy is **0.9323**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd31b89",
   "metadata": {
    "papermill": {
     "duration": 0.057775,
     "end_time": "2023-07-30T03:20:26.331283",
     "exception": false,
     "start_time": "2023-07-30T03:20:26.273508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model vs F1 Fraud Score\n",
    "\n",
    "Compare training and testing fraud F1 scores for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b507197",
   "metadata": {
    "papermill": {
     "duration": 0.367071,
     "end_time": "2023-07-30T03:20:26.756270",
     "exception": false,
     "start_time": "2023-07-30T03:20:26.389199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = [] # Used to create list of tuples.\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    D.append((clf_names[i], train_f1_scores[i], test_f1_scores[i]))\n",
    "       \n",
    "Dsort = sorted(D, key=itemgetter(2), reverse=False) # Sort the list by test F1 score.\n",
    "\n",
    "clf_names_1 = [x[0] for x in Dsort] # Recreate list with sorted classifier (model) names.\n",
    "train_f1_scores = [x[1] for x in Dsort] # Recreate list with sorted false positives. \n",
    "test_f1_scores = [x[2] for x in Dsort] # Recreate list with sorted false negatives. \n",
    "  \n",
    "ind = np.arange(len(clf_names_1)) # Barchart index\n",
    "width = 0.28 \n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = plt.subplot(111)\n",
    "ax.barh(ind, train_f1_scores, width, align='center', color='#D4B9DA', label='Training') \n",
    "ax.barh(ind - width, test_f1_scores, width, align='center', color='#7A0177', label='Testing') \n",
    "ax.set(yticks=ind - width/2, yticklabels=clf_names_1, ylim=[2*width - 1, len(clf_names_1)])\n",
    "plt.xlabel('Fraud F1 Score')\n",
    "plt.title('Fraud Detection - Model vs Fraud F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('img/fraud_f1_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e00de6",
   "metadata": {
    "papermill": {
     "duration": 0.063334,
     "end_time": "2023-07-30T03:20:26.880774",
     "exception": false,
     "start_time": "2023-07-30T03:20:26.817440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Random Forest** model has the highest training and testing fraud F1 score. The training fraud F1 score is **0.9998** and the testing fraud F1 score is **0.8653**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba53ee2",
   "metadata": {
    "papermill": {
     "duration": 0.05749,
     "end_time": "2023-07-30T03:20:26.996162",
     "exception": false,
     "start_time": "2023-07-30T03:20:26.938672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### False Values vs Model\n",
    "\n",
    "Compare the testing data false-positives and false-negatives for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8412d9",
   "metadata": {
    "papermill": {
     "duration": 0.359115,
     "end_time": "2023-07-30T03:20:27.414238",
     "exception": false,
     "start_time": "2023-07-30T03:20:27.055123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = [] # Used to create list of tuples.\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    D.append((clf_names[i], false_pos[i], false_neg[i], false_pos[i]+false_neg[i]))\n",
    "\n",
    "# Sort the list by total false-postives and false-negatives.\n",
    "Dsort = sorted(D, key=itemgetter(3), reverse=False) \n",
    "\n",
    "clf_names_2 = [x[0] for x in Dsort] # Recreate list with sorted classifier (model) names.\n",
    "false_pos = [x[1] for x in Dsort] # Recreate list with sorted false positives. \n",
    "false_neg = [x[2] for x in Dsort] # Recreate list with sorted false negatives. \n",
    "  \n",
    "# Plot bars in stack manner\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(clf_names_2, false_pos, color='green')\n",
    "plt.bar(clf_names_2, false_neg, bottom=false_pos, color='lightgreen')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('False Values')\n",
    "plt.title('Fraud Detection - False Values vs Model')\n",
    "plt.legend(['False Postives', 'False Negatives'])\n",
    "plt.savefig('img/fraud_detection_false_values.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6378128",
   "metadata": {
    "papermill": {
     "duration": 0.060488,
     "end_time": "2023-07-30T03:20:27.533877",
     "exception": false,
     "start_time": "2023-07-30T03:20:27.473389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Random Forest** model has the least amount of misclassified transactions with **84 false-positives** and **193 false-negatives**. This means that this model misclassifed **277 transactions out of the 4094 transactions** (6.766% error) in the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bed1c7",
   "metadata": {
    "papermill": {
     "duration": 0.059587,
     "end_time": "2023-07-30T03:20:27.653572",
     "exception": false,
     "start_time": "2023-07-30T03:20:27.593985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AUC-PR: Area Under The (Precision-Recall) Curve\n",
    "\n",
    "Compare the testing data AUC-PR for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab6355",
   "metadata": {
    "papermill": {
     "duration": 0.440848,
     "end_time": "2023-07-30T03:20:28.154932",
     "exception": false,
     "start_time": "2023-07-30T03:20:27.714084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(rfc_recall, rfc_precision, color='red')\n",
    "plt.plot(ada_recall, ada_precision, color='orange')\n",
    "plt.plot(lrg_recall, lrg_precision, color='blueviolet')\n",
    "plt.plot(dtc_recall, dtc_precision, color='blue')\n",
    "plt.plot(mlp_recall, mlp_precision, color='green')\n",
    "plt.plot(svm_recall, svm_precision, color='dodgerblue')\n",
    "plt.plot(knn_recall, knn_precision, color='magenta')\n",
    "plt.plot(dum_recall, dum_precision, color='black', linestyle='--')\n",
    "plt.legend([f'Random Forest (AUC = {rfc_auc})', \n",
    "            f'AdaBoost (AUC = {ada_auc})',\n",
    "            f'Logistic Regression (AUC = {lrg_auc})', \n",
    "            f'Decision Tree (AUC = {dtc_auc})',\n",
    "            f'MLP (AUC = {mlp_auc})', \n",
    "            f'SVM (AUC = {svm_auc})',\n",
    "            f'KNN (AUC = {knn_auc})', \n",
    "            f'Dummy (AUC = {dum_auc})'])\n",
    "plt.title('PR Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('img/PR_Curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a8f7f",
   "metadata": {
    "papermill": {
     "duration": 0.06401,
     "end_time": "2023-07-30T03:20:28.280392",
     "exception": false,
     "start_time": "2023-07-30T03:20:28.216382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once again, **Random Forest** performed the best with the highest AUC-PR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1824a",
   "metadata": {
    "papermill": {
     "duration": 0.059821,
     "end_time": "2023-07-30T03:20:28.400175",
     "exception": false,
     "start_time": "2023-07-30T03:20:28.340354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-eleven\"></a>\n",
    "# Conclusion\n",
    "\n",
    "All the models performed better than the baseline, but the **Random Forest** model performed the best overall. It has the highest accuracy, F1 score, AUC-PR and it has the least amount of cumulative false values (FP + FN). The **AdaBoost** and **Decision Tree** models are also great choices as well. They both have less false-negatives than the **Random Forest** model, which in my opinion is more consequential than false-positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated",
   "language": "python",
   "name": "federated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33958.03395,
   "end_time": "2023-07-30T03:20:29.353271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-29T17:54:31.319321",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
